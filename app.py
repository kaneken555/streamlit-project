# app.py
# ----------------------------------------
# ğŸ¦™ Ollama Ã— Streamlitï¼ˆæ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼‰
# + æœ€å°RAGï¼ˆChroma + e5/multilingualï¼‰
# ----------------------------------------

import os
import json
import requests
import streamlit as st

# --- RAG ç”¨ ---
import chromadb
from sentence_transformers import SentenceTransformer

# ========================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# ========================================
st.set_page_config(page_title="Ollama Chat", page_icon="ğŸ¦™", layout="centered")
st.title("ğŸ¦™ Ollama Ã— Streamlitï¼ˆæ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ + RAGï¼‰")

# åˆæœŸãƒ¢ãƒ‡ãƒ«ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ä¸Šæ›¸ãã•ã‚Œã‚‹ï¼‰
if "model" not in st.session_state:
    st.session_state.model = "llama3:8b"

# ========================================
# RAGï¼ˆChroma + e5ï¼‰åˆæœŸåŒ–
# - PersistentClient ã§ãƒ­ãƒ¼ã‚«ãƒ«æ°¸ç¶š
# - e5 ã¯æ—¥æœ¬èªã«å¼·ã„å¤šè¨€èªåŸ‹ã‚è¾¼ã¿
# ========================================
@st.cache_resource
def get_vectordb():
    client = chromadb.PersistentClient(path="chroma_db")
    col = client.get_or_create_collection("rag_docs", metadata={"hnsw:space": "cosine"})
    embed = SentenceTransformer("intfloat/multilingual-e5-small")
    return client, col, embed

client, col, embed = get_vectordb()

def retrieve_context(query: str, top_k: int = 6):
    """e5 ã®æ¨å¥¨ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä½¿ã£ã¦æ¤œç´¢â†’ä¸Šä½kä»¶ã‚’é€£çµ"""
    try:
        qvec = embed.encode([f"query: {query}"], normalize_embeddings=True).tolist()[0]
        res = col.query(
            query_embeddings=[qvec],
            n_results=top_k,
            include=["documents", "metadatas", "distances"],
        )
        docs = (res.get("documents") or [[]])[0]
        metas = (res.get("metadatas") or [[]])[0]

        lines, sources = [], []
        for i, (d, m) in enumerate(zip(docs, metas), 1):
            src = (m or {}).get("source", f"doc{i}")
            sources.append(str(src))
            lines.append(f"[{i}] å‡ºå…¸: {src}\n{d}")
        context = "\n\n---\n\n".join(lines)
        return context, sorted(set(sources))
    except Exception:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒç©ºãªã©ã®ã‚±ãƒ¼ã‚¹ã§ã‚‚è½ã¨ã•ãªã„
        return "", []

# ========================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®š
# ========================================
with st.sidebar:
    st.header("âš™ï¸ è¨­å®šï¼ˆOllamaå°‚ç”¨ï¼‰")

    # æ¥ç¶šå…ˆURLï¼ˆãƒã‚¤ãƒ†ã‚£ãƒ–: http://localhost:11434ï¼‰
    default_url = os.environ.get("OLLAMA_URL") or "http://localhost:11434"
    base_url = st.text_input("Ollama URL", value=default_url, help="ä¾‹: http://localhost:11434")

    @st.cache_data(show_spinner=False, ttl=30)
    def list_ollama_models(url: str):
        """Ollama ã® /api/tags ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ï¼ˆå¤±æ•—æ™‚ã¯ç©ºé…åˆ—ï¼‰"""
        try:
            r = requests.get(url.rstrip("/") + "/api/tags", timeout=5)
            r.raise_for_status()
            data = r.json()
            return sorted([m["name"] for m in data.get("models", [])])
        except Exception:
            return []

    models = list_ollama_models(base_url)

    if models:
        init_idx = models.index(st.session_state.model) if st.session_state.model in models else 0
        model = st.selectbox("ãƒ¢ãƒ‡ãƒ«å", models, index=init_idx)
    else:
        st.info("ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚„èµ·å‹•çŠ¶æ…‹ï¼ˆollama serveï¼‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚æ‰‹å…¥åŠ›ã§æŒ‡å®šã§ãã¾ã™ã€‚")
        model = st.text_input("ãƒ¢ãƒ‡ãƒ«åï¼ˆæ‰‹å…¥åŠ›ï¼‰", value=st.session_state.model)

    # é¸æŠ/å…¥åŠ›å€¤ã‚’çŠ¶æ…‹ã¸ä¿å­˜ï¼ˆä¸‹ã®æ¨è«–ã§ä½¿ç”¨ï¼‰
    st.session_state.model = model

    temperature = st.slider("æ¸©åº¦ (å‰µé€ æ€§)", 0.0, 1.0, 0.2, 0.1)
    num_ctx = st.number_input(
        "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•· (num_ctx)", min_value=2048, max_value=32768, value=8192, step=1024,
        help="é•·ãã™ã‚‹ã¨éå»ã®æ–‡è„ˆã‚’ã‚ˆã‚Šä¿æŒï¼ˆãƒ¢ãƒ‡ãƒ«ã®ä¸Šé™ã«æ³¨æ„ï¼‰"
    )

    # æ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®æ—¢å®š System Promptï¼ˆå¿…è¦ã«å¿œã˜ã¦ç·¨é›†ï¼‰
    system_prompt = st.text_area(
        "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        value=(
            "ã‚ãªãŸã¯æ—¥æœ¬èªã§ä¸å¯§ã‹ã¤ã‚ã‹ã‚Šã‚„ã™ãå›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
            "å¿…ãšæ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚è‹±èªã§å‡ºåŠ›ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
            "å°‚é–€ç”¨èªãŒå‡ºã‚‹å ´åˆã¯æ—¥æœ¬èªã®è£œè¶³ã‚‚æ·»ãˆã¦ãã ã•ã„ã€‚\n"
            "éåº¦ã«é•·ãã›ãšã€è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã‚’é©åˆ‡ã«ä½¿ã£ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚"
        ),
        height=140,
    )

    max_history = st.number_input("å±¥æ­´ä¸Šé™ï¼ˆå¾€å¾©æ•°ï¼‰", 2, 50, 10, 1)

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ§¹ å±¥æ­´ã‚¯ãƒªã‚¢"):
            st.session_state.messages = []
            st.rerun()
    with col2:
        st.caption("Dockerã®Streamlitâ†’ãƒã‚¤ãƒ†ã‚£ãƒ–Ollamaã¯ http://host.docker.internal:11434")

# ========================================
# ä¼šè©±ãƒ¡ãƒ¢ãƒª
# ========================================
if "messages" not in st.session_state:
    st.session_state.messages = []

def truncated_history(messages, max_rounds):
    """assistant/user ã®å¾€å¾©ã§ä¸Šé™ã‚’è¶…ãˆãŸã‚‰å¤ã„ã‚‚ã®ã‹ã‚‰å‰Šã‚‹"""
    if len(messages) <= max_rounds * 2:
        return messages
    return messages[-max_rounds * 2:]

# ========================================
# Ollama å‘¼ã³å‡ºã—ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° & ä¾‹å¤–å‡¦ç†ï¼‰
# ========================================
def call_ollama(base_url, prompt, history, model, temperature, system_prompt, num_ctx=8192):
    url = base_url.rstrip("/") + "/api/chat"

    msgs = []
    if system_prompt:
        msgs.append({"role": "system", "content": system_prompt})
    # å±¥æ­´ï¼ˆuser/assistantã®ã¿ï¼‰ï¼‹ä»Šå›ã® user
    msgs += [m for m in history if m["role"] in ("user", "assistant")]
    msgs.append({"role": "user", "content": prompt})

    payload = {
        "model": model,
        "messages": msgs,
        "stream": True,
        "options": {
            "temperature": float(temperature),
            "num_ctx": int(num_ctx),
        },
    }

    try:
        with requests.post(url, json=payload, stream=True, timeout=600) as r:
            r.raise_for_status()
            for line in r.iter_lines():
                if not line:
                    continue
                data = json.loads(line.decode("utf-8"))
                token = data.get("message", {}).get("content", "")
                if token:
                    yield token
    except requests.exceptions.ConnectionError:
        yield "âš ï¸ Ollama ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚URL ã¨èµ·å‹•çŠ¶æ…‹(ollama serve)ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    except requests.exceptions.HTTPError as e:
        yield f"âš ï¸ HTTPã‚¨ãƒ©ãƒ¼: {e.response.status_code} {e.response.text[:200]}"
    except Exception as e:
        yield f"âš ï¸ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}"

# ========================================
# æ—¢å­˜å±¥æ­´ã®æç”»
# ========================================
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ========================================
# å…¥åŠ›å—ä»˜ & å¿œç­”ï¼ˆRAGå·®ã—è¾¼ã¿ãƒã‚¤ãƒ³ãƒˆã‚ã‚Šï¼‰
# ========================================
if user_input := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›â€¦"):
    # æ—¥æœ¬èªå‡ºåŠ›ã‚’å®‰å®šã•ã›ã‚‹è£œåŠ©è¡Œ
    user_input = user_input.strip()

    # ã¾ãšã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ã‚’å±¥æ­´ã¸
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.session_state.messages = truncated_history(st.session_state.messages, max_history)

    with st.chat_message("user"):
        st.markdown(user_input)

    # ---------- â˜… RAG: å‰å‡¦ç†ãƒ»æ¤œç´¢ â˜… ----------
    context, sources = retrieve_context(user_input, top_k=4)
    aug_system_prompt = (
        system_prompt
        + "\n\n# å‚è€ƒè³‡æ–™ï¼ˆæŠœç²‹ï¼‰\n"
        + (context or "ï¼ˆè©²å½“è³‡æ–™ãªã—ï¼‰")
        + "\n\nâ€»ä¸Šè¨˜ã®è³‡æ–™ã®ã¿ã‚’æ ¹æ‹ ã«ã€æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        + "\nå¿…è¦ã«å¿œã˜ã¦ [ç•ªå·] ã‚’ä½¿ã£ã¦æ ¹æ‹ ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚"
    )
    # --------------------------------------------

    with st.chat_message("assistant"):
        stream = call_ollama(
            base_url=base_url,
            prompt=user_input,                 # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            history=st.session_state.messages,
            model=st.session_state.model,      # ã‚µã‚¤ãƒ‰ãƒãƒ¼é¸æŠãƒ¢ãƒ‡ãƒ«
            temperature=temperature,
            system_prompt=aug_system_prompt,   # RAGæ–‡è„ˆã‚’åŒæ¢±
            num_ctx=num_ctx,
        )
        reply = st.write_stream(stream)

    # ç”Ÿæˆçµæœã‚’ä¼šè©±ãƒ¡ãƒ¢ãƒªã¸
    st.session_state.messages.append({"role": "assistant", "content": reply})

    # å‡ºå…¸ã®è¡¨ç¤ºï¼ˆä»»æ„ï¼‰
    if sources:
        st.caption("å‡ºå…¸: " + " | ".join(sources))
